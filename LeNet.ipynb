{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_df.values/255\n",
    "y = train_df.label.values\n",
    "x = train_df.iloc[:,1:].values/255\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, test_size = 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTorch_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "trainTorch_y = torch.from_numpy(train_y).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "valTorch_x = torch.from_numpy(val_x).type(torch.FloatTensor)\n",
    "valTorch_y = torch.from_numpy(val_y).type(torch.LongTensor) \n",
    "\n",
    "testTorch_x = torch.from_numpy(np.array(x_test)).type(torch.FloatTensor)\n",
    "# testTorch_x = testTorch_x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(trainTorch_x, trainTorch_y)\n",
    "val = torch.utils.data.TensorDataset(valTorch_x, valTorch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size = 100, shuffle = False)\n",
    "val_loader = DataLoader(val, batch_size = 100, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,6,kernel_size = 5,padding = 2)\n",
    "        self.conv2 = torch.nn.Conv2d(6,16,kernel_size = 5)\n",
    "        self.fc1 = torch.nn.Linear(16*5*5,120)\n",
    "        self.fc2 = torch.nn.Linear(120,84)\n",
    "        self.fc3 = torch.nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "#         x为张量，张量.size 取出维度  取0  得到就是样本数量 n 1 28 28\n",
    "        x = x.view(batch_size,1,28,28)\n",
    "        x = F.max_pool2d( F.relu(self.conv1(x)) , 2)\n",
    "        x = F.max_pool2d( F.relu(self.conv2(x)) , 2)\n",
    "        \n",
    "        x = x.view(batch_size,-1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "# model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr = 0.01,momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将单轮循环封装为一个函数\n",
    "\n",
    "def train_func(epoch):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx,data in enumerate(train_loader,0):\n",
    "        inputs,target = data\n",
    "#         inputs,target = inputs.to(device),target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "#         forward and backward and update\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d,%5d] loss: %.3f' % (epoch + 1,batch_idx + 1,running_loss / 300))\n",
    "            running_loss =0.0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "#         执行之后在下面代码就不会执行梯度\n",
    "        for data in val_loader:\n",
    "            images,labels = data\n",
    "#             images,labels = images.to(device),labels.to(device)\n",
    "#             拿数据\n",
    "            outputs = model(images)\n",
    "#             做预测，拿到的结果是一个矩阵，每一行都是一个独热向量\n",
    "            _, predicted = torch.max(outputs.data,dim = 1)\n",
    "#           返回 最大值 和 每一行的最大值下标\n",
    "#           指定沿着维度1（往下 行是第0个维度，向右 列是第一个维度）\n",
    "            total += labels.size(0)\n",
    "#             label是一个N 1元组 size 取 0 就是？\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func():\n",
    "    with torch.no_grad():\n",
    "        outputs = model(testTorch_x)\n",
    "        index, predicted = torch.max(outputs.data,dim = 1)\n",
    "    ans2 = predicted\n",
    "    submission_file2 = pd.read_csv('./sample_submission.csv')\n",
    "    submission_file2.Label = ans2\n",
    "    submission_file2.to_csv('submission_cnn_lenet.csv', index=False)\n",
    "    print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  300] loss: 1.266\n",
      "Accuracy on test set: 84 %\n",
      "[2,  300] loss: 0.373\n",
      "Accuracy on test set: 90 %\n",
      "[3,  300] loss: 0.263\n",
      "Accuracy on test set: 92 %\n",
      "[4,  300] loss: 0.204\n",
      "Accuracy on test set: 94 %\n",
      "[5,  300] loss: 0.163\n",
      "Accuracy on test set: 95 %\n",
      "[6,  300] loss: 0.136\n",
      "Accuracy on test set: 95 %\n",
      "[7,  300] loss: 0.118\n",
      "Accuracy on test set: 96 %\n",
      "[8,  300] loss: 0.105\n",
      "Accuracy on test set: 96 %\n",
      "[9,  300] loss: 0.095\n",
      "Accuracy on test set: 96 %\n",
      "[10,  300] loss: 0.087\n",
      "Accuracy on test set: 96 %\n",
      "[11,  300] loss: 0.081\n",
      "Accuracy on test set: 97 %\n",
      "[12,  300] loss: 0.075\n",
      "Accuracy on test set: 97 %\n",
      "[13,  300] loss: 0.071\n",
      "Accuracy on test set: 97 %\n",
      "[14,  300] loss: 0.066\n",
      "Accuracy on test set: 97 %\n",
      "[15,  300] loss: 0.062\n",
      "Accuracy on test set: 97 %\n",
      "[16,  300] loss: 0.059\n",
      "Accuracy on test set: 97 %\n",
      "[17,  300] loss: 0.056\n",
      "Accuracy on test set: 97 %\n",
      "[18,  300] loss: 0.053\n",
      "Accuracy on test set: 97 %\n",
      "[19,  300] loss: 0.050\n",
      "Accuracy on test set: 97 %\n",
      "[20,  300] loss: 0.048\n",
      "Accuracy on test set: 97 %\n",
      "[21,  300] loss: 0.045\n",
      "Accuracy on test set: 97 %\n",
      "[22,  300] loss: 0.043\n",
      "Accuracy on test set: 97 %\n",
      "[23,  300] loss: 0.041\n",
      "Accuracy on test set: 97 %\n",
      "[24,  300] loss: 0.039\n",
      "Accuracy on test set: 97 %\n",
      "[25,  300] loss: 0.037\n",
      "Accuracy on test set: 97 %\n",
      "[26,  300] loss: 0.036\n",
      "Accuracy on test set: 97 %\n",
      "[27,  300] loss: 0.034\n",
      "Accuracy on test set: 97 %\n",
      "[28,  300] loss: 0.033\n",
      "Accuracy on test set: 97 %\n",
      "[29,  300] loss: 0.031\n",
      "Accuracy on test set: 97 %\n",
      "[30,  300] loss: 0.030\n",
      "Accuracy on test set: 97 %\n",
      "[31,  300] loss: 0.028\n",
      "Accuracy on test set: 97 %\n",
      "[32,  300] loss: 0.027\n",
      "Accuracy on test set: 97 %\n",
      "[33,  300] loss: 0.026\n",
      "Accuracy on test set: 97 %\n",
      "[34,  300] loss: 0.025\n",
      "Accuracy on test set: 97 %\n",
      "[35,  300] loss: 0.024\n",
      "Accuracy on test set: 98 %\n",
      "[36,  300] loss: 0.023\n",
      "Accuracy on test set: 97 %\n",
      "[37,  300] loss: 0.022\n",
      "Accuracy on test set: 98 %\n",
      "[38,  300] loss: 0.021\n",
      "Accuracy on test set: 98 %\n",
      "[39,  300] loss: 0.020\n",
      "Accuracy on test set: 98 %\n",
      "[40,  300] loss: 0.019\n",
      "Accuracy on test set: 98 %\n",
      "[41,  300] loss: 0.018\n",
      "Accuracy on test set: 98 %\n",
      "[42,  300] loss: 0.018\n",
      "Accuracy on test set: 98 %\n",
      "[43,  300] loss: 0.017\n",
      "Accuracy on test set: 98 %\n",
      "[44,  300] loss: 0.016\n",
      "Accuracy on test set: 98 %\n",
      "[45,  300] loss: 0.015\n",
      "Accuracy on test set: 98 %\n",
      "[46,  300] loss: 0.015\n",
      "Accuracy on test set: 98 %\n",
      "[47,  300] loss: 0.014\n",
      "Accuracy on test set: 98 %\n",
      "[48,  300] loss: 0.013\n",
      "Accuracy on test set: 98 %\n",
      "[49,  300] loss: 0.013\n",
      "Accuracy on test set: 98 %\n",
      "[50,  300] loss: 0.012\n",
      "Accuracy on test set: 98 %\n",
      "[51,  300] loss: 0.011\n",
      "Accuracy on test set: 98 %\n",
      "[52,  300] loss: 0.011\n",
      "Accuracy on test set: 98 %\n",
      "[53,  300] loss: 0.010\n",
      "Accuracy on test set: 98 %\n",
      "[54,  300] loss: 0.010\n",
      "Accuracy on test set: 98 %\n",
      "[55,  300] loss: 0.009\n",
      "Accuracy on test set: 98 %\n",
      "[56,  300] loss: 0.009\n",
      "Accuracy on test set: 98 %\n",
      "[57,  300] loss: 0.008\n",
      "Accuracy on test set: 98 %\n",
      "[58,  300] loss: 0.008\n",
      "Accuracy on test set: 98 %\n",
      "[59,  300] loss: 0.007\n",
      "Accuracy on test set: 98 %\n",
      "[60,  300] loss: 0.007\n",
      "Accuracy on test set: 98 %\n",
      "[61,  300] loss: 0.007\n",
      "Accuracy on test set: 98 %\n",
      "[62,  300] loss: 0.006\n",
      "Accuracy on test set: 98 %\n",
      "[63,  300] loss: 0.006\n",
      "Accuracy on test set: 98 %\n",
      "[64,  300] loss: 0.006\n",
      "Accuracy on test set: 98 %\n",
      "[65,  300] loss: 0.005\n",
      "Accuracy on test set: 98 %\n",
      "[66,  300] loss: 0.005\n",
      "Accuracy on test set: 98 %\n",
      "[67,  300] loss: 0.005\n",
      "Accuracy on test set: 98 %\n",
      "[68,  300] loss: 0.005\n",
      "Accuracy on test set: 98 %\n",
      "[69,  300] loss: 0.004\n",
      "Accuracy on test set: 98 %\n",
      "[70,  300] loss: 0.004\n",
      "Accuracy on test set: 98 %\n",
      "[71,  300] loss: 0.004\n",
      "Accuracy on test set: 98 %\n",
      "[72,  300] loss: 0.004\n",
      "Accuracy on test set: 98 %\n",
      "[73,  300] loss: 0.004\n",
      "Accuracy on test set: 98 %\n",
      "[74,  300] loss: 0.003\n",
      "Accuracy on test set: 98 %\n",
      "[75,  300] loss: 0.003\n",
      "Accuracy on test set: 98 %\n",
      "[76,  300] loss: 0.003\n",
      "Accuracy on test set: 98 %\n",
      "[77,  300] loss: 0.003\n",
      "Accuracy on test set: 98 %\n",
      "[78,  300] loss: 0.003\n",
      "Accuracy on test set: 98 %\n",
      "[79,  300] loss: 0.003\n",
      "Accuracy on test set: 98 %\n",
      "[80,  300] loss: 0.003\n",
      "Accuracy on test set: 98 %\n",
      "[81,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[82,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[83,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[84,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[85,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[86,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[87,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[88,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[89,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[90,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[91,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[92,  300] loss: 0.002\n",
      "Accuracy on test set: 98 %\n",
      "[93,  300] loss: 0.001\n",
      "Accuracy on test set: 98 %\n",
      "[94,  300] loss: 0.001\n",
      "Accuracy on test set: 98 %\n",
      "[95,  300] loss: 0.001\n",
      "Accuracy on test set: 98 %\n",
      "[96,  300] loss: 0.001\n",
      "Accuracy on test set: 98 %\n",
      "[97,  300] loss: 0.001\n",
      "Accuracy on test set: 98 %\n",
      "[98,  300] loss: 0.001\n",
      "Accuracy on test set: 98 %\n",
      "[99,  300] loss: 0.001\n",
      "Accuracy on test set: 98 %\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for epoch in range(99):\n",
    "        train_func(epoch)\n",
    "        test_func()\n",
    "    predict_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
